```
# ===================================================
# == COMPREHENSIVE NOTES ON THE PANDAS LIBRARY ==
# ===================================================
#
# Date: August 5, 2025

#
# This document contains elaborated notes on the Pandas library,
# formatted for easy copying and pasting.

# ===== INTRODUCTION =====

# --- What is Pandas? ---
# Pandas is an open-source Python library that has become the standard tool for data manipulation, analysis, and cleaning.
# It provides high-performance, easy-to-use data structures and data analysis tools that are essential
# for any data scientist, analyst, or engineer working with data in Python. The name "Pandas" is derived
# from "Panel Data," an econometrics term for multidimensional data.

# --- Why Use Pandas? ---
# * Handles Tabular Data: It excels at working with structured, tabular data (like spreadsheets or SQL tables).
# * Data Cleaning: Provides a powerful toolkit for handling messy, incomplete, and inconsistent data.
# * Performance: It is built on top of NumPy, meaning many operations are vectorized and fast.
# * Integration: It integrates seamlessly with other data science libraries like Matplotlib, Seaborn, and Scikit-learn.


# ===== 1. INSTALLATION AND IMPORTING =====

# First, you need to install the library using pip.
# In your terminal or command prompt, run:
# pip install pandas

# In your Python scripts, it is a universal convention to import Pandas with the alias `pd`.
import pandas as pd
import numpy as np # Often imported alongside pandas


# ===== 2. CORE DATA STRUCTURES =====

# Pandas has two primary data structures that you will use constantly.

# --- A. The `Series` ---
# A `Series` is a one-dimensional, labeled array capable of holding any data type.
# It's like a single column in a spreadsheet.
# It consists of Data and an Index.

# Creating a `Series`:
# From a list
s = pd.Series([10, 20, 30, 40], name='MyNumbers')
print("# --- Series from list ---")
print(s)

# With a custom index
s_custom = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
print("\n# --- Series with custom index ---")
print(s_custom)


# --- B. The `DataFrame` ---
# A `DataFrame` is a two-dimensional, labeled data structure with columns of potentially different types.
# It is the most commonly used Pandas object. Think of it as an Excel spreadsheet or an SQL table.
# It consists of Data, an Index (for rows), and Columns.

# Creating a `DataFrame`: The most common way is from a dictionary of lists.
data = {
    'State': ['Kerala', 'Tamil Nadu', 'Karnataka', 'Maharashtra'],
    'Capital': ['Thiruvananthapuram', 'Chennai', 'Bengaluru', 'Mumbai'],
    'Population_M': [35.7, 77.8, 67.9, 126.4]
}

df = pd.DataFrame(data)
print("\n# --- DataFrame from Dictionary ---")
print(df)


# ===== 3. READING AND WRITING DATA =====

# Pandas can read and write data from a wide variety of formats. CSV is the most common.

# Reading data from a CSV file into a DataFrame
# We'll create a dummy file first, then read it.
df.to_csv('countries_for_reading.csv', index=False)
df_from_csv = pd.read_csv('countries_for_reading.csv')
print("\n# --- DataFrame read from countries_for_reading.csv ---")
print(df_from_csv.head())


# Writing a DataFrame to a new CSV file
# index=False prevents pandas from writing the DataFrame index as a column
df.to_csv('new_countries_output.csv', index=False)

# Other common formats:
# df.to_excel('output.xlsx', index=False)
# df_excel = pd.read_excel('input.xlsx')
# df.to_json('output.json')
# df_json = pd.read_json('input.json')


# ===== 4. DATA INSPECTION AND EXPLORATION =====

# Once you have a DataFrame, the first step is to understand it.

print("\n# --- Inspecting Data ---")
# View the first 5 rows
print("\ndf.head():\n", df.head())

# View the last 3 rows
print("\ndf.tail(3):\n", df.tail(3))

# Get a concise summary of the DataFrame
print("\ndf.info():")
df.info()

# Generate descriptive statistics for numerical columns
print("\ndf.describe():\n", df.describe())

# Get the dimensions of the DataFrame (rows, columns)
print("\nShape:", df.shape)

# Get the data types of each column
print("\nData Types:\n", df.dtypes)

# Get the count of unique values in a specific column
print("\nValue Counts for State:\n", df['State'].value_counts())


# ===== 5. DATA SELECTION AND INDEXING =====

# This is a critical area for subsetting your data.

# --- A. Basic Selection ---
# Selecting Columns: Use square brackets `[]`.
# Select a single column (returns a Series)
capitals_series = df['Capital']

# Select multiple columns (returns a DataFrame)
state_and_pop_df = df[['State', 'Population_M']]
print("\n# --- Selecting Multiple Columns ---")
print(state_and_pop_df.head())


# --- B. Label-Based Selection: `.loc` ---
# Use `.loc` to select data by its row and column LABELS.
# df.loc[row_labels, column_labels]
df_indexed = df.set_index('State') # Set 'State' as the index to use string labels

# Select the row with the index label 'Karnataka'
print("\n# --- .loc selection by label 'Karnataka' ---")
print(df_indexed.loc['Karnataka'])

# Select multiple rows and specific columns
print("\n# --- .loc multiple rows and columns ---")
print(df_indexed.loc[['Kerala', 'Maharashtra'], ['Capital', 'Population_M']])


# --- C. Position-Based Selection: `.iloc` ---
# Use `.iloc` to select data by its INTEGER POSITION.
# df.iloc[row_positions, column_positions]
# Note: We use the original `df` here which has a default integer index.
# Select the first row (at position 0)
print("\n# --- .iloc selection for first row ---")
print(df.iloc[0])

# Select the first 3 rows and the first 2 columns
print("\n# --- .iloc for slice of rows/cols ---")
print(df.iloc[0:3, 0:2])


# --- D. Conditional Selection (Boolean Indexing) ---
# This is the most powerful way to filter data based on conditions.
# Get all rows where Population_M is greater than 100
large_pop_df = df[df['Population_M'] > 100]
print("\n# --- Conditional Selection (Population > 100) ---")
print(large_pop_df)

# Combine multiple conditions using & (AND) and | (OR)
# IMPORTANT: Each condition must be wrapped in parentheses ().
southern_large_pop = df[(df['Population_M'] > 50) & (df['State'].isin(['Tamil Nadu', 'Karnataka']))]
print("\n# --- Multiple Conditions ---")
print(southern_large_pop)


# ===== 6. DATA MANIPULATION AND CLEANING =====

# --- A. Adding and Removing Columns ---
df['Region'] = ['South', 'South', 'South', 'West'] # Add a new column
df['Population_in_Cr'] = df['Population_M'] / 10 # Create column from existing data
print("\n# --- DataFrame with new columns ---")
print(df.head())

# Drop a column (axis=1 specifies column)
df_dropped = df.drop('Population_in_Cr', axis=1)


# --- B. Handling Missing Data (NaN) ---
data_with_nan = {
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, np.nan, 8],
    'C': [10, 20, 30, 40]
}
df_nan = pd.DataFrame(data_with_nan)
print("\n# --- DataFrame with NaN values ---")
print(df_nan)

# Count null values per column
print("\n# --- Count of NaN values ---")
print(df_nan.isnull().sum())

# Drop rows containing any null values
print("\n# --- After dropna() ---")
print(df_nan.dropna())

# Fill null values with a specific value (e.g., 0)
print("\n# --- After fillna(0) ---")
print(df_nan.fillna(value=0))


# ===== 7. GROUPING AND AGGREGATION (`groupby`) =====

# The `groupby` operation involves Splitting, Applying a function, and Combining results.
# Group the DataFrame by the 'Region' column
grouped_by_region = df.groupby('Region')

# Calculate the mean of numerical columns for each region
print("\n# --- Groupby Region and find mean ---")
print(grouped_by_region.mean(numeric_only=True))

# Perform multiple aggregations at once using .agg()
print("\n# --- Groupby and aggregate sum, mean, count ---")
print(grouped_by_region['Population_M'].agg(['sum', 'mean', 'count']))


# ===== 8. MERGING, JOINING, AND CONCATENATING =====

# These operations allow you to combine multiple DataFrames.
# `pd.merge()`: Combines DataFrames based on common columns, similar to an SQL JOIN.

# Sample DataFrames for merging
df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['B', 'C', 'D'], 'value2': [4, 5, 6]})

# Inner Join (only keeps keys present in BOTH DataFrames)
inner_join = pd.merge(df1, df2, on='key', how='inner')
print("\n# --- Inner Join ---")
print(inner_join)

# Left Join (keeps all keys from the left DataFrame)
left_join = pd.merge(df1, df2, on='key', how='left')
print("\n# --- Left Join ---")
print(left_join)

# Outer Join (keeps all keys from both DataFrames)
outer_join = pd.merge(df1, df2, on='key', how='outer')
print("\n# --- Outer Join ---")
print(outer_join)

```